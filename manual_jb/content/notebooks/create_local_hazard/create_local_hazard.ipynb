{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Local Hazards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial provides examples of how to create local hazards for tornadoes, earthquakes, tsunamis, floods, and hurricanes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:23:46.971951Z",
     "start_time": "2023-10-05T17:23:46.967669Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyincore import Hurricane, Flood, Earthquake, Tornado\n",
    "from pyincore.models.hazard.tsunami import Tsunami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to indicate the local data path initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tornadoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs**: JSON representation of a dataset describing a tornado. Each available dataset in Shapefile format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir, \"tornado_dataset.json\"), 'r') as file:\n",
    "    tornado_dataset_json = file.read()\n",
    "    print(json.dumps(json.loads(tornado_dataset_json), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tornado object\n",
    "tornado = Tornado.from_json_file(os.path.join(dir, \"tornado_dataset.json\"))\n",
    "\n",
    "# attach dataset from local file\n",
    "tornado.hazardDatasets[0].from_file((os.path.join(dir, \"joplin_tornado/joplin_path_wgs84.shp\")), \n",
    "                                    data_type=\"incore:tornadoWindfield\")\n",
    "\n",
    "payload = [\n",
    "    {\n",
    "        \"demands\": [\"wind\"],\n",
    "        \"units\": [\"mph\"],\n",
    "        \"loc\": \"37.04, -94.37\"\n",
    "    }\n",
    "]\n",
    "\n",
    "values = tornado.read_hazard_values(payload, seed=1234) # removing the seed will give random values\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs**: JSON representation of a dataset describing an earthquake. Each available dataset in TIF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir, \"eq-dataset.json\"), 'r') as file:\n",
    "    earthquake_dataset_json = file.read()\n",
    "    print(json.dumps(json.loads(earthquake_dataset_json), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the earthquake object\n",
    "eq = Earthquake.from_json_file(os.path.join(dir, \"eq-dataset.json\"))\n",
    "\n",
    "# attach datasets from local files\n",
    "eq.hazardDatasets[0].from_file((os.path.join(dir, \"eq-dataset-SA.tif\")),\n",
    "                               data_type=\"ergo:probabilisticEarthquakeRaster\")\n",
    "eq.hazardDatasets[1].from_file((os.path.join(dir, \"eq-dataset-PGA.tif\")),\n",
    "                               data_type=\"ergo:probabilisticEarthquakeRaster\")\n",
    "\n",
    "payload = [\n",
    "    {\n",
    "        \"demands\": [\"PGA\", \"0.2 SA\"],\n",
    "        \"units\": [\"g\", \"g\"],\n",
    "        \"loc\": \"35.03,-89.93\"\n",
    "    }\n",
    "]\n",
    "\n",
    "values = eq.read_hazard_values(payload)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tsunamis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs**: JSON representation of a dataset describing a tsunami. Each available dataset in TIF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir, \"tsunami.json\"), 'r') as file:\n",
    "    tsunami_dataset_json = file.read()\n",
    "    print(json.dumps(json.loads(tsunami_dataset_json), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tsunami object\n",
    "tsunami = Tsunami.from_json_file(os.path.join(dir, \"tsunami.json\"))\n",
    "\n",
    "# attach datasets from local files\n",
    "tsunami.hazardDatasets[0].from_file((os.path.join(dir, \"Tsu_100yr_Vmax.tif\")),\n",
    "                                    data_type=\"ncsa:probabilisticTsunamiRaster\")\n",
    "tsunami.hazardDatasets[1].from_file((os.path.join(dir, \"Tsu_100yr_Mmax.tif\")),\n",
    "                                    data_type=\"ncsa:probabilisticTsunamiRaster\")\n",
    "tsunami.hazardDatasets[2].from_file((os.path.join(dir, \"Tsu_100yr_Hmax.tif\")),\n",
    "                                    data_type=\"ncsa:probabilisticTsunamiRaster\")\n",
    "\n",
    "payload = [\n",
    "    {\n",
    "        \"demands\": [\"hmax\"],\n",
    "        \"units\": [\"m\"],\n",
    "        \"loc\": \"46.006,-123.935\"\n",
    "    }\n",
    "]\n",
    "\n",
    "values = tsunami.read_hazard_values(payload)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Floods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs**: JSON representation of a dataset describing a flood. Each available dataset in TIF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir, \"flood-dataset.json\"), 'r') as file:\n",
    "    flood_dataset_json = file.read()\n",
    "    print(json.dumps(json.loads(flood_dataset_json), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the flood object\n",
    "flood = Flood.from_json_file(os.path.join(dir, \"flood-dataset.json\"))\n",
    "\n",
    "# attach datasets from local files\n",
    "flood.hazardDatasets[0].from_file((os.path.join(dir, \"flood-inundationDepth-50ft.tif\")),\n",
    "                                  data_type=\"ncsa:probabilisticFloodRaster\")\n",
    "flood.hazardDatasets[1].from_file(os.path.join(dir, \"flood-WSE-50ft.tif\"),\n",
    "                                  data_type=\"ncsa:probabilisticFloodRaster\")\n",
    "\n",
    "payload = [\n",
    "    {\n",
    "         \"demands\": [\"waterSurfaceElevation\"],\n",
    "         \"units\": [\"m\"],\n",
    "         \"loc\": \"34.60,-79.16\"\n",
    "     }\n",
    " ]\n",
    "\n",
    "values = flood.read_hazard_values(payload)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hurricanes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs**: JSON representation of a dataset describing a hurricane. Each available dataset in TIF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(dir, \"hurricane-dataset.json\"), 'r') as file:\n",
    "    hurricane_dataset_json = file.read()\n",
    "    print(json.dumps(json.loads(hurricane_dataset_json), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the hurricane object\n",
    "hurricane = Hurricane.from_json_file((os.path.join(dir, \"hurricane-dataset.json\")))\n",
    "\n",
    "# attach datasets from local files\n",
    "hurricane.hazardDatasets[0].from_file((os.path.join(dir, \"Wave_Raster.tif\")),\n",
    "                                      data_type=\"ncsa:deterministicHurricaneRaster\")\n",
    "hurricane.hazardDatasets[1].from_file(os.path.join(dir, \"Surge_Raster.tif\"),\n",
    "                                      data_type=\"ncsa:deterministicHurricaneRaster\")\n",
    "hurricane.hazardDatasets[2].from_file(os.path.join(dir, \"Inundation_Raster.tif\"),\n",
    "                                      data_type=\"ncsa:deterministicHurricaneRaster\")\n",
    "\n",
    "payload = [\n",
    "        {\n",
    "            \"demands\": [\"waveHeight\", \"surgeLevel\"],\n",
    "            \"units\": [\"m\", \"m\"],\n",
    "            \"loc\": \"29.22,-95.06\"\n",
    "        },\n",
    "        {\n",
    "            \"demands\": [\"waveHeight\", \"surgeLevel\"],\n",
    "            \"units\": [\"cm\", \"cm\"],\n",
    "            \"loc\": \"29.23,-95.05\"\n",
    "        },\n",
    "        {\n",
    "            \"demands\": [\"waveHeight\", \"inundationDuration\"],\n",
    "            \"units\": [\"in\", \"hr\"],\n",
    "            \"loc\": \"29.22,-95.06\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "values = hurricane.read_hazard_values(payload)\n",
    "print(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
