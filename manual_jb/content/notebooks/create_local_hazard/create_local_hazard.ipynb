{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Local Hazards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial provides examples of how to create local hazards for tornadoes, earthquakes, tsunamis, floods, and hurricanes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:23:46.971951Z",
     "start_time": "2023-10-05T17:23:46.967669Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyincore import Hurricane, Flood, Earthquake, Tornado\n",
    "from pyincore.models.hazard.tsunami import Tsunami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to indicate the local data path initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tornadoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs**: JSON representation of a dataset describing a tornado. Each available dataset in Shapefile format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"pytest - Joplin Tornado\",\n",
      "    \"description\": \"Joplin tornado hazard\",\n",
      "    \"tornadoType\": \"dataset\",\n",
      "    \"threshold\": null,\n",
      "    \"thresholdUnit\": \"mph\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(dir, \"tornado_dataset.json\"), 'r') as file:\n",
    "    tornado_dataset_json = file.read()\n",
    "    print(json.dumps(json.loads(tornado_dataset_json), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'demands': ['wind'], 'units': ['mph'], 'loc': '37.04, -94.37', 'hazardValues': [102.62435891472981]}]\n"
     ]
    }
   ],
   "source": [
    "# create the tornado object\n",
    "tornado = Tornado.from_json_file(os.path.join(dir, \"tornado_dataset.json\"))\n",
    "\n",
    "# attach dataset from local file\n",
    "tornado.hazardDatasets[0].from_file((os.path.join(dir, \"joplin_tornado/joplin_path_wgs84.shp\")), \n",
    "                                    data_type=\"incore:tornadoWindfield\")\n",
    "\n",
    "payload = [\n",
    "    {\n",
    "        \"demands\": [\"wind\"],\n",
    "        \"units\": [\"mph\"],\n",
    "        \"loc\": \"37.04, -94.37\"\n",
    "    }\n",
    "]\n",
    "\n",
    "values = tornado.read_hazard_values(payload, seed=1234) # removing the seed will give random values\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs**: JSON representation of a dataset describing an earthquake. Each available dataset in TIF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"pytest - Memphis Deterministic EQ\",\n",
      "    \"description\": \"Memphis dataset based deterministic hazard - Pytest\",\n",
      "    \"eqType\": \"dataset\",\n",
      "    \"hazardDatasets\": [\n",
      "        {\n",
      "            \"hazardType\": \"deterministic\",\n",
      "            \"demandType\": \"SA\",\n",
      "            \"demandUnits\": \"g\",\n",
      "            \"period\": \"0.2\",\n",
      "            \"eqParameters\": {\n",
      "                \"srcLatitude\": \"35.927\",\n",
      "                \"srcLongitude\": \"-89.919\",\n",
      "                \"magnitude\": \"7.9\",\n",
      "                \"depth\": \"10.0\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"hazardType\": \"deterministic\",\n",
      "            \"demandType\": \"PGA\",\n",
      "            \"demandUnits\": \"g\",\n",
      "            \"period\": \"0.0\",\n",
      "            \"eqParameters\": {\n",
      "                \"srcLatitude\": \"35.927\",\n",
      "                \"srcLongitude\": \"-89.919\",\n",
      "                \"magnitude\": \"7.9\",\n",
      "                \"depth\": \"10.0\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(dir, \"eq-dataset.json\"), 'r') as file:\n",
    "    earthquake_dataset_json = file.read()\n",
    "    print(json.dumps(json.loads(earthquake_dataset_json), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'demands': ['PGA', '0.2 SA'], 'units': ['g', 'g'], 'loc': '35.03,-89.93', 'hazardValues': [0.3149999976158142, 0.4729999899864197]}]\n"
     ]
    }
   ],
   "source": [
    "# create the earthquake object\n",
    "eq = Earthquake.from_json_file(os.path.join(dir, \"eq-dataset.json\"))\n",
    "\n",
    "# attach datasets from local files\n",
    "eq.hazardDatasets[0].from_file((os.path.join(dir, \"eq-dataset-SA.tif\")),\n",
    "                               data_type=\"ergo:probabilisticEarthquakeRaster\")\n",
    "eq.hazardDatasets[1].from_file((os.path.join(dir, \"eq-dataset-PGA.tif\")),\n",
    "                               data_type=\"ergo:probabilisticEarthquakeRaster\")\n",
    "\n",
    "payload = [\n",
    "    {\n",
    "        \"demands\": [\"PGA\", \"0.2 SA\"],\n",
    "        \"units\": [\"g\", \"g\"],\n",
    "        \"loc\": \"35.03,-89.93\"\n",
    "    }\n",
    "]\n",
    "\n",
    "values = eq.read_hazard_values(payload)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tsunamis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs**: JSON representation of a dataset describing a tsunami. Each available dataset in TIF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"pytest - Seaside Probabilistic Tsunami - 100 yr\",\n",
      "    \"description\": \"pytest - Seaside dataset based probabilistic tsunami hazard. This is just a test!\",\n",
      "    \"tsunamiType\": \"dataset\",\n",
      "    \"hazardDatasets\": [\n",
      "        {\n",
      "            \"hazardType\": \"probabilistic\",\n",
      "            \"demandType\": \"Vmax\",\n",
      "            \"demandUnits\": \"m/s\",\n",
      "            \"recurrenceInterval\": \"100\",\n",
      "            \"recurrenceUnit\": \"years\"\n",
      "        },\n",
      "        {\n",
      "            \"hazardType\": \"probabilistic\",\n",
      "            \"demandType\": \"Mmax\",\n",
      "            \"demandUnits\": \"m^3/s^2\",\n",
      "            \"recurrenceInterval\": \"100\",\n",
      "            \"recurrenceUnit\": \"years\"\n",
      "        },\n",
      "        {\n",
      "            \"hazardType\": \"probabilistic\",\n",
      "            \"demandType\": \"Hmax\",\n",
      "            \"demandUnits\": \"m\",\n",
      "            \"recurrenceInterval\": \"100\",\n",
      "            \"recurrenceUnit\": \"years\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(dir, \"tsunami.json\"), 'r') as file:\n",
    "    tsunami_dataset_json = file.read()\n",
    "    print(json.dumps(json.loads(tsunami_dataset_json), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'demands': ['hmax'], 'units': ['m'], 'loc': '46.006,-123.935', 'hazardValues': [2.9000000953674316]}]\n"
     ]
    }
   ],
   "source": [
    "# create the tsunami object\n",
    "tsunami = Tsunami.from_json_file(os.path.join(dir, \"tsunami.json\"))\n",
    "\n",
    "# attach datasets from local files\n",
    "tsunami.hazardDatasets[0].from_file((os.path.join(dir, \"Tsu_100yr_Vmax.tif\")),\n",
    "                                    data_type=\"ncsa:probabilisticTsunamiRaster\")\n",
    "tsunami.hazardDatasets[1].from_file((os.path.join(dir, \"Tsu_100yr_Mmax.tif\")),\n",
    "                                    data_type=\"ncsa:probabilisticTsunamiRaster\")\n",
    "tsunami.hazardDatasets[2].from_file((os.path.join(dir, \"Tsu_100yr_Hmax.tif\")),\n",
    "                                    data_type=\"ncsa:probabilisticTsunamiRaster\")\n",
    "\n",
    "payload = [\n",
    "    {\n",
    "        \"demands\": [\"hmax\"],\n",
    "        \"units\": [\"m\"],\n",
    "        \"loc\": \"46.006,-123.935\"\n",
    "    }\n",
    "]\n",
    "\n",
    "values = tsunami.read_hazard_values(payload)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Floods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs**: JSON representation of a dataset describing a flood. Each available dataset in TIF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"Lumberton Deterministic Flood - riverine flooding\",\n",
      "    \"description\": \"Lumberton dataset based deterministic hazard - 2 datasets\",\n",
      "    \"floodType\": \"dataset\",\n",
      "    \"hazardDatasets\": [\n",
      "        {\n",
      "            \"hazardType\": \"deterministic\",\n",
      "            \"demandType\": \"inundationDepth\",\n",
      "            \"demandUnits\": \"ft\",\n",
      "            \"floodParameters\": {\n",
      "                \"model\": \"riverine flooding\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"hazardType\": \"deterministic\",\n",
      "            \"demandType\": \"waterSurfaceElevation\",\n",
      "            \"demandUnits\": \"ft\",\n",
      "            \"floodParameters\": {\n",
      "                \"model\": \"riverine flooding\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(dir, \"flood-dataset.json\"), 'r') as file:\n",
    "    flood_dataset_json = file.read()\n",
    "    print(json.dumps(json.loads(flood_dataset_json), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'demands': ['waterSurfaceElevation'], 'units': ['m'], 'loc': '34.60,-79.16', 'hazardValues': [41.970442822265625]}]\n"
     ]
    }
   ],
   "source": [
    "# create the flood object\n",
    "flood = Flood.from_json_file(os.path.join(dir, \"flood-dataset.json\"))\n",
    "\n",
    "# attach datasets from local files\n",
    "flood.hazardDatasets[0].from_file((os.path.join(dir, \"flood-inundationDepth-50ft.tif\")),\n",
    "                                  data_type=\"ncsa:probabilisticFloodRaster\")\n",
    "flood.hazardDatasets[1].from_file(os.path.join(dir, \"flood-WSE-50ft.tif\"),\n",
    "                                  data_type=\"ncsa:probabilisticFloodRaster\")\n",
    "\n",
    "payload = [\n",
    "    {\n",
    "         \"demands\": [\"waterSurfaceElevation\"],\n",
    "         \"units\": [\"m\"],\n",
    "         \"loc\": \"34.60,-79.16\"\n",
    "     }\n",
    " ]\n",
    "\n",
    "values = flood.read_hazard_values(payload)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hurricanes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs**: JSON representation of a dataset describing a hurricane. Each available dataset in TIF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"pytest - Galveston Deterministic Hurricane - Kriging \",\n",
      "    \"description\": \"Galveston dataset based deterministic hazard - 3 datasets\",\n",
      "    \"hurricaneType\": \"dataset\",\n",
      "    \"hazardDatasets\": [\n",
      "        {\n",
      "            \"hazardType\": \"deterministic\",\n",
      "            \"demandType\": \"waveHeight\",\n",
      "            \"demandUnits\": \"m\",\n",
      "            \"hurricaneParameters\": {\n",
      "                \"model\": \"Kriging\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"hazardType\": \"deterministic\",\n",
      "            \"demandType\": \"surgeLevel\",\n",
      "            \"demandUnits\": \"m\",\n",
      "            \"hurricaneParameters\": {\n",
      "                \"model\": \"Kriging\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"hazardType\": \"deterministic\",\n",
      "            \"demandType\": \"inundationDuration\",\n",
      "            \"demandUnits\": \"hr\",\n",
      "            \"hurricaneParameters\": {\n",
      "                \"model\": \"Kriging\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(dir, \"hurricane-dataset.json\"), 'r') as file:\n",
    "    hurricane_dataset_json = file.read()\n",
    "    print(json.dumps(json.loads(hurricane_dataset_json), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'demands': ['waveHeight', 'surgeLevel'], 'units': ['m', 'm'], 'loc': '29.22,-95.06', 'hazardValues': [1.54217780024576, 3.663398872786693]}, {'demands': ['waveHeight', 'surgeLevel'], 'units': ['cm', 'cm'], 'loc': '29.23,-95.05', 'hazardValues': [162.9628933899723, 369.7690109274975]}, {'demands': ['waveHeight', 'inundationDuration'], 'units': ['in', 'hr'], 'loc': '29.22,-95.06', 'hazardValues': [60.7156942134556, 18.346923306935572]}]\n"
     ]
    }
   ],
   "source": [
    "# create the hurricane object\n",
    "hurricane = Hurricane.from_json_file((os.path.join(dir, \"hurricane-dataset.json\")))\n",
    "\n",
    "# attach datasets from local files\n",
    "hurricane.hazardDatasets[0].from_file((os.path.join(dir, \"Wave_Raster.tif\")),\n",
    "                                      data_type=\"ncsa:deterministicHurricaneRaster\")\n",
    "hurricane.hazardDatasets[1].from_file(os.path.join(dir, \"Surge_Raster.tif\"),\n",
    "                                      data_type=\"ncsa:deterministicHurricaneRaster\")\n",
    "hurricane.hazardDatasets[2].from_file(os.path.join(dir, \"Inundation_Raster.tif\"),\n",
    "                                      data_type=\"ncsa:deterministicHurricaneRaster\")\n",
    "\n",
    "payload = [\n",
    "        {\n",
    "            \"demands\": [\"waveHeight\", \"surgeLevel\"],\n",
    "            \"units\": [\"m\", \"m\"],\n",
    "            \"loc\": \"29.22,-95.06\"\n",
    "        },\n",
    "        {\n",
    "            \"demands\": [\"waveHeight\", \"surgeLevel\"],\n",
    "            \"units\": [\"cm\", \"cm\"],\n",
    "            \"loc\": \"29.23,-95.05\"\n",
    "        },\n",
    "        {\n",
    "            \"demands\": [\"waveHeight\", \"inundationDuration\"],\n",
    "            \"units\": [\"in\", \"hr\"],\n",
    "            \"loc\": \"29.22,-95.06\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "values = hurricane.read_hazard_values(payload)\n",
    "print(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
