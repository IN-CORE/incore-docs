{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMSA: Seismic Functionality and Restoration Analysis for Interdependent Buildings-Water-Power using Restoration Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MMSA was selected as a testbed for research studies by the NIST Center for Risk-Based Community Resilience Planning to test algorithms developed for community resilience assessment on a large urban area with a diverse population and economy. The population of Memphis, Shelby County, and MMSA, respectively, are about 0.7 M, 1.0 M, and 1.4 M. This dense population lies within the most seismically active area of Central and Eastern United States, called the New Madrid Seismic Zone (NMSZ), which is capable of producing large damaging earthquakes. \n",
    "\n",
    "This notebook perform seismic damage and functionality analysis of interdependent buildings, electric power and water distribution network systems of Shelby County, TN. First the geospatial infrastructure inventory and service area dataset are imported to characterize the buildings and lifeline systems of the MMSA testbed and the interdependency between infrastructure systems. Then, scenario earthquakes obtained from a ground motion prediction model to generate spatial intensity measures for the infrastructure network area and perform damage analysis to obtain component-level damage probability estimates subject to the scenario earthquake.This is followed by developing a fragility-based computational model of the networked infrastructure using graph theory where components are modeled as nodes, and the connection between nodes are modeled as directed links. The outcome of component level structural damage assessment is used to perform functionality and restoration analysis of buildings and networks by considering the interdependency between infrastructure systems and the cascading failure of each network component using an input-output model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook was developed by Milad Roohi (CSU/UNL - milad.roohi@unl.edu) in colaboration with Jiate Li (CSU) and John W. van de Lindt (CSU), the NCSA team (Jong Sung Lee, Chris Navarro, Diego Calderon, Chen Wang, Michal Ondrejcek, Gowtham Naraharisetty)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Atkinson GM, Boore DM. (1995) Ground-motion relations for eastern North America. Bulletin of the Seismological Society of America. 1995 Feb 1;85(1):17-30.\n",
    "\n",
    "[2] Elnashai, A.S., Cleveland, L.J., Jefferson, T., and Harrald, J., (2008). “Impact of Earthquakes on the Central USA” A Report of the Mid-America Earthquake Center. http://mae.cee.illinois.edu/publications/publications_reports.html![image.png](attachment:97c026ce-8248-481d-b109-e48fd108e38e.png)![image.png](attachment:a4706ad2-73ed-430a-ab45-4c6f56307d29.png)![image.png](attachment:6ec6d200-f0bb-45a8-b6c1-3acc03f041fa.png)![image.png](attachment:1772e5a6-1560-433f-b3e0-60edc7d68d90.png)\n",
    "\n",
    "[3] Linger S, Wolinsky M. (2001) Los Alamos National Laboratory Report LA-UR-01-3361 ESRI Paper No. 889 Estimating Electrical Service Areas Using GIS and Cellular Automata.\n",
    "\n",
    "[4] Roohi M, van de Lindt JW, Rosenheim N, Hu Y, Cutler H. (2021) Implication of building inventory accuracy on physical and socio-economic resilience metrics for informed decision-making in natural hazards. Structure and Infrastructure Engineering. 2020 Nov 20;17(4):534-54.\n",
    "\n",
    "[5] Federal Emergency Management Agency (FEMA). (2020) Hazus Earthquake Model Technical Manual. 2020 Oct.\n",
    "\n",
    "[6] Haimes YY, Horowitz BM, Lambert JH, Santos JR, Lian C, Crowther KG. (2005) Inoperability input-output model for interdependent infrastructure sectors. I: Theory and methodology. Journal of Infrastructure Systems. 2005 Jun;11(2):67-79.\n",
    "\n",
    "[7] Milad Roohi, Jiate Li, John van de Lindt. (2022) Seismic Functionality Analysis of Interdependent Buildings and Lifeline Systems 12th National Conference on Earthquake Engineering (12NCEE), Salt Lake City, UT (June 27-July 1, 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - utils.py:_init_num_threads() - Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO - utils.py:_init_num_threads() - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyincore_viz'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/wl/mt7kxmjd643_t7sszdww6yjm0000gp/T/ipykernel_11746/2467253901.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyincore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0manalyses\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwaterfacilitydamage\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mWaterFacilityDamage\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyincore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0manalyses\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepfdamage\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mEpfDamage\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyincore_viz\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgeoutil\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mGeoUtil\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mviz\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyincore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0manalyses\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmontecarlofailureprobability\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mMonteCarloFailureProbability\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pyincore_viz'"
     ]
    }
   ],
   "source": [
    "from pyincore import HazardService, IncoreClient, Dataset, FragilityService, MappingSet, DataService, SpaceService\n",
    "from pyincore.analyses.buildingdamage import BuildingDamage\n",
    "from pyincore.analyses.pipelinedamagerepairrate import PipelineDamageRepairRate\n",
    "from pyincore.analyses.waterfacilitydamage import WaterFacilityDamage\n",
    "from pyincore.analyses.epfdamage import EpfDamage\n",
    "from pyincore_viz.geoutil import GeoUtil as viz\n",
    "from pyincore.analyses.montecarlofailureprobability import MonteCarloFailureProbability\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "\n",
    "import copy\n",
    "\n",
    "from scipy.stats import poisson,bernoulli\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "client = IncoreClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Define Earthquake Hazard Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario earthquakes are created using Atkinson and Boore (1995) [1] GMPE model including earthquakes with magnitudes $M_w5.9$, $M_w6.5$ and $M_w7.1$ to study the effect of seismic hazard intensity in resilience metrics. The scenario can be selected by defining selecting earthquake magnitude using \"select_EQ_magnitude\" parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define hazard type\n",
    "hazard_type = \"earthquake\"\n",
    "\n",
    "#### Select the earthquake scenario magnitude from the following 'EQ_hazard_dict' dictionary keys\n",
    "select_EQ_magnitude = 7.1\n",
    "\n",
    "#### Define hazard dictionary with three ground motion intensity levels\n",
    "EQ_hazard_dict = {}\n",
    "EQ_hazard_dict[5.9] = {'id': \"5e3db155edc9fa00085d7c09\", 'name':'M59'}\n",
    "EQ_hazard_dict[6.5] = {'id': \"5e45a1308591b700088c799c\", 'name':'M65'}\n",
    "EQ_hazard_dict[7.1] = {'id': \"5e3dd04f7fdf7e0008032bfe\", 'name':'M71'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define filename and create folder for saving results\n",
    "hazard_id = EQ_hazard_dict[select_EQ_magnitude]['id']\n",
    "hazard_name = EQ_hazard_dict[select_EQ_magnitude]['name']\n",
    "\n",
    "import os\n",
    "current_path = os.getcwd()\n",
    "\n",
    "#### create folder \"MMSA_analysis_results\" to save results\n",
    "results_folder = \"MMSA_analysis_results\"\n",
    "if not os.path.isdir(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "#### define folder path for each eathquake magnitude to save results in different folders\n",
    "fp = current_path + '/' + results_folder + '/' + hazard_name\n",
    "if not os.path.isdir(fp):\n",
    "    os.makedirs(fp)\n",
    "fp = fp + '/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Define Infrastructure Inventory Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1) Building Inventory Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mid-America Earthquake (MAE) center [2] initiated the Memphis testbed (MTB) project to demonstrate the seismic risk assessment to the civil infrastructure system in Shelby County, TN. As part of that project, a high-resolution model of the physical and social infrastructure was developed to investigate the potential impact due to earthquake activity in the New Madrid Seismic Zone (NMSZ) (Elnashai et al., 2008). \n",
    "\n",
    "The building inventory is defined based on the Shelby county shapefile available on the IN-CORE data service (ID: 5a284f0bc7d30d13bc081a46). This study considers all the buildings with a total number of 306003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Building inventory dataset for Shelby county, TN\n",
    "bldg_dataset_id = \"5a284f0bc7d30d13bc081a46\"  \n",
    "\n",
    "# #### Import building dataset from IN-CORE data service\n",
    "bldg_dataset = Dataset.from_data_service(bldg_dataset_id, DataService(client))\n",
    "\n",
    "# #### Convert building dataset to geodataframe\n",
    "bldg_gdf = bldg_dataset.get_dataframe_from_shapefile()\n",
    "bldg_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structural system type statistics of MMSA building inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot= bldg_gdf.pivot_table(\n",
    "     index='struct_typ',\n",
    "     values='guid',\n",
    "     aggfunc=np.count_nonzero\n",
    ")\n",
    "pivot.columns = ['Count']\n",
    "pivot.sort_values('Count',ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Plot building dataset \n",
    "viz.plot_map(bldg_dataset, column='struct_typ',category='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2) Electric Power Network (EPN) Inventory Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read EPN node and link inventory data\n",
    "df_EPNnodes  = gpd.read_file(\"shapefiles/Mem_power_link5_node.shp\")\n",
    "df_EPNlinks  = gpd.read_file(\"shapefiles/Mem_power_link5.shp\")\n",
    "\n",
    "### Plot EPN shapefiles\n",
    "ax = df_EPNnodes.plot(figsize=(20, 10), column='utilfcltyc', categorical=True, markersize=150, legend=True,)\n",
    "df_EPNlinks.plot(ax=ax, color='k')\n",
    "cx.add_basemap(ax, crs=df_EPNnodes.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3) Water Distribution System (WDS) Inventory Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read WDS node and link inventory data\n",
    "df_WDSnodes  = gpd.read_file(\"shapefiles/Mem_water_node5.shp\")\n",
    "df_WDSlinks  = gpd.read_file(\"shapefiles/Mem_water_pipeline_node_Modified.shp\")\n",
    "\n",
    "### Plot WDS shapefiles\n",
    "ax = df_WDSnodes.plot(figsize=(20, 10), column='utilfcltyc', categorical=True, markersize=150, legend=True,)\n",
    "df_WDSlinks.plot(ax=ax, color='k')\n",
    "cx.add_basemap(ax, crs=df_WDSnodes.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4) EPN service area dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interdependency between buildings and utility network infrastructure is modeled by using a Cellular Automata algorithm [3] to estimate the service areas of each network component and perform geospatial analysis to identify the buildings located within the service area of each component. A CSV file (\"Memphis_EPN_Bldg_Depend.csv\") has been developd that maps each of the EPN substations to corresponsing buildings within the node's service area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the service are mapping between EPN nodes (i.e., sguid) and buildings (i.e., guid)\n",
    "filepath = 'Inputs/Memphis_EPN_Bldg_Depend.csv'\n",
    "df_EPN_Bldg_Depend = pd.read_csv(filepath, index_col=False)\n",
    "df_EPN_Bldg_Depend = df_EPN_Bldg_Depend.rename(columns={'Bldg_guid': 'guid', 'Substation_guid':'sguid'})\n",
    "df_EPN_Bldg_Depend.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Infrastructure Damage Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1) Buildings Physical Damage, MC and Functionality Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The \"BuildingDamage\" module of pyincore is used for building damage analysis. This module requires specifying the building inventory data cases and fragility mapping set, scenario hazard tye ad ID and subsequently run analysis to estimate probability of exceeding various damage states. \n",
    "\n",
    "The fragility mappings are performed based on the value of five attributes, including the number of stories (“no_stories”), the occupancy type (“occ_type”), the year built (“year_built”), the structural type (“struct_typ”), and “efacility”. Once the rules in the mapping file (ID=5b47b2d9337d4a36187c7564) are satisfied, mapping IDs from the fragility service are returned to map fragility parameters to each node of the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1-A) Damage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import building damage analyis module from pyincore.\n",
    "from pyincore.analyses.buildingdamage import BuildingDamage  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bldg_dmg = BuildingDamage(client)\n",
    "\n",
    "#### Load building input dataset\n",
    "bldg_dmg.load_remote_input_dataset(\"buildings\", bldg_dataset_id) \n",
    "\n",
    "#### Load fragility mapping\n",
    "fragility_service = FragilityService(client)\n",
    "mapping_id = \"5b47b2d9337d4a36187c7564\"\n",
    "\n",
    "bldg_mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "bldg_dmg.set_input_dataset('dfr3_mapping_set', bldg_mapping_set)\n",
    "\n",
    "#### Set analysis parameters\n",
    "result_name = fp + \"1_MMSA_building_damage_result\"\n",
    "bldg_dmg.set_parameter(\"result_name\", result_name)\n",
    "bldg_dmg.set_parameter(\"hazard_type\", hazard_type)\n",
    "bldg_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "bldg_dmg.set_parameter(\"num_cpu\", 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run building damage analysis\n",
    "bldg_dmg.run_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Obtain the building damage results\n",
    "#building_dmg_result = bldg_dmg.get_output_dataset('ds_result')\n",
    "\n",
    "#### Convert the building damage results to dataframe\n",
    "#df_bldg_dmg = building_dmg_result.get_dataframe_from_csv()\n",
    "\n",
    "### Remove empty rows and produce a modified dataset\n",
    "#df_bldg_dmg = df_bldg_dmg.dropna()\n",
    "\n",
    "#df_bldg_dmg_mod = Dataset.from_dataframe(df_bldg_dmg,\n",
    "#                                         name=\"df_bldg_dmg_mod\",\n",
    "#                                         data_type=\"ergo:buildingDamageVer6\")\n",
    "\n",
    "#df_bldg_dmg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: The building damage analysis results subject to Mw7.1 is imported from a CSV file but for the final release the previous code cells can be uncommented and use for damage analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be read directly from pipeline\n",
    "building_dmg_result = Dataset.from_file(fp+\"1_MMSA_bldg_dmg_result.csv\", data_type=\"ergo:buildingDamageVer4\")\n",
    "df_bldg_dmg = building_dmg_result.get_dataframe_from_csv()\n",
    "df_bldg_dmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add 'DS_max' attribute to df_bldg_dmg that provide the most probable damage state for each MMSA building\n",
    "df_bldg_dmg['DS_max'] = df_bldg_dmg.loc[:,['DS_0', 'DS_1', 'DS_2', 'DS_3']].idxmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Plot of the distribution of most probable damage state for buildings\n",
    "indexes = df_bldg_dmg['DS_max'].value_counts(normalize=True).mul(100).index.tolist()\n",
    "values = df_bldg_dmg['DS_max'].value_counts(normalize=True).mul(100).tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 2.5), dpi=200)\n",
    "\n",
    "bars = ax.bar(x=indexes, height=values,)\n",
    "\n",
    "for bar in bars:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 3,f'% {bar.get_height() :.1f}',\n",
    "            horizontalalignment='center')\n",
    "\n",
    "fig.tight_layout()\n",
    "ax.set_xlabel('Damage State', labelpad=15)\n",
    "ax.set_ylabel('Percentage', labelpad=15)\n",
    "ax.set_title('Distribution of most probable damage state for buildings', pad=15)\n",
    "ax.set(frame_on=False);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1-B) Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Monte Carlo simulation (MCS) approach is employed to estimate the failure probability for each. The MCS has been widely recognized as a powerful modeling tool in risk and reliability literature to solve mathematical problems using random samples, which allows capturing the uncertainty in the damage estimation process. The MCS begins with sampling a vector r from uniform distribution U(0,1), where the length of random vector r is the number of Monte Carlo samples given by N. The samples are compared with probabilities of all damage states corresponding to hazard intensity measures to determine the damage state of each component. Subsequently, the number of samples experiencing damage state 2 and higher is calculated and the failure probability is approximated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyincore.analyses.montecarlofailureprobability import MonteCarloFailureProbability \n",
    "\n",
    "num_samples = 100 # Require 500 samples for convergence - Selected smaller samples for testing \n",
    "result_name = fp + \"2_MMSA_mc_failure_probability_buildings\"\n",
    "\n",
    "mc_bldg = MonteCarloFailureProbability(client)\n",
    "\n",
    "mc_bldg.set_input_dataset(\"damage\", building_dmg_result)\n",
    "mc_bldg.set_parameter(\"num_cpu\", 8)\n",
    "mc_bldg.set_parameter(\"num_samples\", num_samples)\n",
    "mc_bldg.set_parameter(\"damage_interval_keys\", [\"DS_0\", \"DS_1\", \"DS_2\", \"DS_3\"])\n",
    "mc_bldg.set_parameter(\"failure_state_keys\", [\"DS_1\", \"DS_2\", \"DS_3\"])\n",
    "\n",
    "mc_bldg.set_parameter(\"result_name\", result_name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_bldg.run_analysis() \n",
    "\n",
    "# Obtain buildings failure probabilities\n",
    "building_failure_probability = mc_bldg.get_output_dataset('failure_probability')  \n",
    "\n",
    "df_bldg_fail = building_failure_probability.get_dataframe_from_csv()\n",
    "df_bldg_fail.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_damage_mcs_samples = mc_bldg.get_output_dataset('sample_failure_state')  # get buildings failure states\n",
    "\n",
    "bdmcs = building_damage_mcs_samples.get_dataframe_from_csv()\n",
    "bdmcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1-C) Functionality Analysis (Buildings Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MC samples from previous subsection are used in this subsection to perform functionality analysis and determine the functionality state of each building. According to Almufti and Willford (2013), functionality is defined as the capacity of a component to serve its intended objectives consist of structural integrity, safety, and utilities (e.g., water and electricity). An individual building can be narrowly classified into five discrete states consist of the restricted entry (State 1), restricted use (State 2), preoccupancy (State 3), baseline functionality (State 4) and full functionality (State 5). In a broader classification, functionality states can be categorized into nonfunctional (States 1 to 3) and functional (States 4 and 5). This notebook relies on the latter broader classification approach to estimate the functionality state of each building and subsequently use functionality estimates to perfom functionality analysis by accounting for interdependency between buildings and lifeline networks (i.e., EPN and WDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyincore.analyses.buildingfunctionality import BuildingFunctionality\n",
    "bldg_func = BuildingFunctionality(client)\n",
    "\n",
    "# Load the datasets of MC simulations for buildings\n",
    "bldg_func.set_input_dataset(\"building_damage_mcs_samples\", building_damage_mcs_samples)\n",
    "\n",
    "result_name = fp + \"2_MMSA_mcs_functionality_probability\"\n",
    "bldg_func.set_parameter(\"result_name\", result_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the BuildingFunctionality module to obtain the building functionality probabilities\n",
    "bldg_func.run_analysis() \n",
    "\n",
    "bldg_func_samples = bldg_func.get_output_dataset('functionality_samples')\n",
    "df_bldg_samples = bldg_func_samples.get_dataframe_from_csv()\n",
    "df_bldg_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bldg_func_probability = bldg_func.get_output_dataset('functionality_probability')\n",
    "df_bldg_func = bldg_func_probability.get_dataframe_from_csv()\n",
    "df_bldg_func = df_bldg_func.rename(columns={\"building_guid\": \"guid\"})\n",
    "func_prob_target = 0.40\n",
    "df_bldg_func.loc[df_bldg_func['probability'] <= func_prob_target, 'functionality'] = 0 # Non-Functional\n",
    "df_bldg_func.loc[df_bldg_func['probability'] > func_prob_target, 'functionality'] = 1 # Functional\n",
    "df_bldg_func.loc[df_bldg_func['probability'] <= func_prob_target, 'functionality_state'] = 'Non-Functional' # Non-Functional\n",
    "df_bldg_func.loc[df_bldg_func['probability'] > func_prob_target, 'functionality_state'] = 'Functional' # Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bldg_func.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot of the distribution of most probable damage state for buildings\n",
    "indexes = df_bldg_func['functionality_state'].value_counts(normalize=True).mul(100).index.tolist()\n",
    "values = df_bldg_func['functionality_state'].value_counts(normalize=True).mul(100).tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 2.5), dpi=200)\n",
    "\n",
    "bars = ax.bar(x=indexes, height=values,)\n",
    "\n",
    "for bar in bars:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 3,f'% {bar.get_height() :.1f}',\n",
    "            horizontalalignment='center')\n",
    "\n",
    "fig.tight_layout()\n",
    "ax.set_ylim([0,100])\n",
    "ax.set_xlabel('Damage State', labelpad=15)\n",
    "ax.set_ylabel('Percentage', labelpad=15)\n",
    "ax.set_title('Functionality Percentage (Buildings only)', pad=15)\n",
    "ax.set(frame_on=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2) Electric Power Network (EPN) Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section perform damage analysis of EPN substations based on Hazus fragility functions [5].\n",
    "\n",
    "Subsequntly, restoration curves of electric substations from Hazus is used to obtain functionality percentage of each substation. The output of this analysis provides the percentage of building withing each substation service area that will suffer power outage 1 day, 3 days, 7 days, 30 days and 90 days after an event.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-2-A) EPN Damage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyincore.analyses.epfdamage.epfdamage import EpfDamage # Import epf damage module integrated into pyIncore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_id = \"5b47be62337d4a37b6197a3a\" \n",
    "\n",
    "fragility_service = FragilityService(client)\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "\n",
    "epn_sub_dmg = EpfDamage(client)\n",
    "epn_dataset = Dataset.from_file(\"shapefiles/Mem_power_link5_node.shp\", data_type=\"incore:epf\")\n",
    "epn_sub_dmg.set_input_dataset(\"epfs\", epn_dataset)\n",
    "epn_sub_dmg.set_input_dataset(\"dfr3_mapping_set\", mapping_set)\n",
    "\n",
    "result_name = fp + \"3_MMSA_EPN_substations_dmg_result\"\n",
    "epn_sub_dmg.set_parameter(\"result_name\", result_name)\n",
    "epn_sub_dmg.set_parameter(\"hazard_type\", hazard_type)\n",
    "epn_sub_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "epn_sub_dmg.set_parameter(\"num_cpu\", 16)\n",
    "\n",
    "epn_sub_dmg.run_analysis()\n",
    "\n",
    "substation_dmg_result = epn_sub_dmg.get_output_dataset('result')\n",
    "\n",
    "df_sub_dmg = substation_dmg_result.get_dataframe_from_csv()\n",
    "df_sub_dmg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epf_df = epn_dataset.get_dataframe_from_shapefile()\n",
    "df_sub_result = pd.merge(epf_df, df_sub_dmg, on='guid', how='outer')\n",
    "df_sub_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_epf_dmg = df_sub_result.groupby(by=['utilfcltyc'], as_index=True)\\\n",
    ".agg({'DS_0': 'mean', 'DS_1':'mean', 'DS_2': 'mean', 'DS_3': 'mean', 'DS_4': 'mean', 'guid': 'count'})\n",
    "grouped_epf_dmg.rename(columns={'guid': 'total_count'}, inplace=True)\n",
    "ax = grouped_epf_dmg[[\"DS_0\", \"DS_1\", \"DS_2\", \"DS_3\", \"DS_4\"]].plot.barh(stacked=True)\n",
    "ax.set_title(\"Stacked Bar Chart of Damage State Grouped by Structure Type\", fontsize=12)\n",
    "ax.set_xlabel(\"complete damage value\", fontsize=12)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "grouped_epf_dmg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-2-B) EPN Monte Carlo Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyincore.analyses.montecarlofailureprobability import MonteCarloFailureProbability\n",
    "\n",
    "num_samples = 10000\n",
    "mc_sub = MonteCarloFailureProbability(client)\n",
    "\n",
    "result_name = fp + \"3_MMSA_EPN_substations_mc_failure_probability\"\n",
    "mc_sub.set_input_dataset(\"damage\", substation_dmg_result)\n",
    "mc_sub.set_parameter(\"num_cpu\", 16)\n",
    "mc_sub.set_parameter(\"num_samples\", num_samples)\n",
    "mc_sub.set_parameter(\"damage_interval_keys\", [\"DS_0\", \"DS_1\", \"DS_2\", \"DS_3\", \"DS_4\"])\n",
    "mc_sub.set_parameter(\"failure_state_keys\", [\"DS_3\", \"DS_4\"])\n",
    "\n",
    "mc_sub.set_parameter(\"result_name\", result_name) # name of csv file with results\n",
    "mc_sub.run_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substation_failure_probability = mc_sub.get_output_dataset('failure_probability')  # get buildings failure probabilities\n",
    "\n",
    "df_substation_fail = substation_failure_probability.get_dataframe_from_csv()\n",
    "df_substation_fail.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substation_damage_mcs_samples = mc_sub.get_output_dataset('sample_failure_state')\n",
    "\n",
    "sdmcs = substation_damage_mcs_samples.get_dataframe_from_csv()\n",
    "sdmcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3) Water Distribution System Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section perform damage analysis of WDS including water facility and pipelines. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-3-A) Water Facility (WF) Damage Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hazus fragility functions used to analyze the water facilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water facility inventory for Shelby County, TN\n",
    "facility_dataset_id = \"5a284f2ac7d30d13bc081e52\" # Mem_water_node5.dbf = 2 node types\n",
    "\n",
    "# Default water facility fragility mapping\n",
    "#mapping_id = \"5b47c3b1337d4a387e85564b\"  # Hazus Potable Water Facility Fragility Mapping - Only PGA\n",
    "mapping_id = \"5b47c383337d4a387669d592\" #Potable Water Facility Fragility Mapping for INA - Has PGD\n",
    "fragility_key = \"pga\"\n",
    "\n",
    "# Liquefaction parameters\n",
    "liq_geology_dataset_id =  \"5a284f53c7d30d13bc08249c\"\n",
    "liquefaction = False\n",
    "liq_fragility_key = \"pgd\"\n",
    "\n",
    "# Hazard uncertainty\n",
    "uncertainty = False\n",
    "facility_dataset = Dataset.from_data_service(facility_dataset_id, DataService(client))\n",
    "df_inv_facility = facility_dataset.get_dataframe_from_shapefile()\n",
    "\n",
    "print(df_inv_facility.utilfcltyc.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inv_facility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create water facility damage analysis\n",
    "wf_dmg = WaterFacilityDamage(client)\n",
    "\n",
    "# Load water facility inventory dataset\n",
    "wf_dmg.load_remote_input_dataset(\"water_facilities\", facility_dataset_id)\n",
    "\n",
    "# Load fragility mapping\n",
    "fragility_service = FragilityService(client)\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "wf_dmg.set_input_dataset(\"dfr3_mapping_set\", mapping_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specify result name\n",
    "result_name = fp + \"2_MMSA_facility_dmg_result\"\n",
    "\n",
    "# Set analysis parameters\n",
    "wf_dmg.set_parameter(\"result_name\", result_name)\n",
    "wf_dmg.set_parameter(\"hazard_type\", hazard_type)\n",
    "wf_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "wf_dmg.set_parameter(\"fragility_key\", fragility_key)\n",
    "wf_dmg.set_parameter(\"use_liquefaction\", liquefaction)\n",
    "wf_dmg.set_parameter(\"liquefaction_geology_dataset_id\", liq_geology_dataset_id)\n",
    "wf_dmg.set_parameter(\"liquefaction_fragility_key\", liq_fragility_key)\n",
    "wf_dmg.set_parameter(\"use_hazard_uncertainty\", uncertainty)\n",
    "wf_dmg.set_parameter(\"num_cpu\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run water facility damage analysis\n",
    "wf_dmg.run_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfacility_dmg_result = wf_dmg.get_output_dataset(\"result\")\n",
    "\n",
    "# Convert dataset to Pandas DataFrame\n",
    "df_waterfacility_dmg = waterfacility_dmg_result.get_dataframe_from_csv()\n",
    "df_waterfacility_dmg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-3-B) Water Facility (WF) MC Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_wf = MonteCarloFailureProbability(client)\n",
    "num_samples = 10000\n",
    "mc_wf.set_input_dataset(\"damage\", waterfacility_dmg_result)\n",
    "mc_wf.set_parameter(\"result_name\", \"wf_dmg_mc\")\n",
    "mc_wf.set_parameter(\"num_cpu\", 8)\n",
    "mc_wf.set_parameter(\"num_samples\", num_samples)\n",
    "mc_wf.set_parameter(\"damage_interval_keys\", [\"DS_0\", \"DS_1\", \"DS_2\", \"DS_3\", \"DS_4\"])\n",
    "mc_wf.set_parameter(\"failure_state_keys\", [\"DS_3\", \"DS_4\"])\n",
    "mc_wf.run_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get water facility failure probabilities\n",
    "waterfacility_failure_probability = mc_wf.get_output_dataset('failure_probability')  \n",
    "df_waterfacility_fail = waterfacility_failure_probability.get_dataframe_from_csv()\n",
    "df_waterfacility_fail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3-3-C) Pipeline Damage and Servicability Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines are analyzed using PipelineDamageRepairRate module of pyincore and subsequently the servicability index for each pipeline is estimated using Hazus procedure [5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Water Buried Pipeline inventory in Shelby county, TN\n",
    "pipeline_dataset_id = \"5a284f28c7d30d13bc081d14\"\n",
    "\n",
    "# pipeline fragility mapping\n",
    "mapping_id = \"5b47c227337d4a38464efea8\"\n",
    "\n",
    "# Geology dataset\n",
    "liq_geology_dataset_id = \"5a284f53c7d30d13bc08249c\"\n",
    "liq_fragility_key = \"pgd\"\n",
    "\n",
    "use_liq = True\n",
    "use_liq = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_dataset = Dataset.from_data_service(pipeline_dataset_id, DataService(client))\n",
    "df_inv_pipeline = pipeline_dataset.get_dataframe_from_shapefile()\n",
    "df_inv_pipeline[\"x\"] = df_inv_pipeline.centroid.map(lambda p: p.x)\n",
    "df_inv_pipeline[\"y\"] = df_inv_pipeline.centroid.map(lambda p: p.y)\n",
    "df_inv_pipeline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create pipeline damage with repair rate\n",
    "pipeline_dmg_w_rr = PipelineDamageRepairRate(client)\n",
    "\n",
    "# Load pipeline inventory as input datasets\n",
    "pipeline_dmg_w_rr.load_remote_input_dataset(\"pipeline\", pipeline_dataset_id)\n",
    "\n",
    "# Load fragility mapping\n",
    "fragility_service = FragilityService(client)\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "pipeline_dmg_w_rr.set_input_dataset(\"dfr3_mapping_set\", mapping_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the result name\n",
    "result_name = fp + \"2_MMSA_pipeline_dmg_result\"\n",
    "\n",
    "# Set analysis parameters\n",
    "pipeline_dmg_w_rr.set_parameter(\"result_name\", result_name)\n",
    "pipeline_dmg_w_rr.set_parameter(\"hazard_type\", hazard_type)\n",
    "pipeline_dmg_w_rr.set_parameter(\"hazard_id\", hazard_id)\n",
    "pipeline_dmg_w_rr.set_parameter(\"liquefaction_fragility_key\", liq_fragility_key)\n",
    "pipeline_dmg_w_rr.set_parameter(\"liquefaction_geology_dataset_id\",liq_geology_dataset_id)\n",
    "pipeline_dmg_w_rr.set_parameter(\"use_liquefaction\", use_liq)\n",
    "pipeline_dmg_w_rr.set_parameter(\"num_cpu\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run pipeline  damage analysis\n",
    "pipeline_dmg_w_rr.run_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve result dataset\n",
    "pipline_dmg_result = pipeline_dmg_w_rr.get_output_dataset(\"result\")\n",
    "\n",
    "# Convert dataset to Pandas DataFrame\n",
    "df_pipline_dmg = pipline_dmg_result.get_dataframe_from_csv()\n",
    "df_pipline_dmg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WDS_links = pd.merge(df_WDSlinks, df_pipline_dmg, on='guid', how='outer')\n",
    "df_WDSlinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pipelines, Hazus [5] assumes two damage states consist of: leaks and breaks. When a pipe is damaged due to ground failure (PGD), the type of damage is likely to be a break, while when a pipe is damaged due to seismic wave propagation (PGV), the type of damage is likely to be joint pull-out or crushing at the bell, which generally cause leaks. In the Hazus Methodology, it is assumed that damage due to seismic waves will consist of 80% leaks and 20% breaks, while damage due to ground failure will consist of 20% leaks and 80% breaks. Servicability index can be calculated using the following equation\n",
    "\n",
    "$$ SI = 1 - Lognormal(ARR, 0.1, 0.85) $$\n",
    "\n",
    "Where SI is servicability index, ARR is average repair rate and the lognormal function has a median of 0.1 repairs/km and a beta of 0.85, and . Please refer to section 8.1.7 of Hazs (Water System Performance) for further information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import math\n",
    "for idx in df_WDSlinks['linknwid']:\n",
    "    df = df_WDSlinks[df_WDSlinks.linknwid.isin([idx])]\n",
    "\n",
    "    # standard deviation of normal distribution\n",
    "    sigma = 0.85\n",
    "    \n",
    "    # mean of normal distribution\n",
    "    mu = math.log(.1)\n",
    "    \n",
    "    C_pgv = 0.2 # 0.2\n",
    "    C_pgd = 0.8 # 0.8\n",
    "    im = (C_pgv * df['numpgvrpr'] + C_pgd * df['numpgdrpr']).sum()/df['length'].sum()\n",
    "    SI_break = 1-stats.lognorm(s=sigma, scale=math.exp(mu)).cdf(im)\n",
    "    \n",
    "    C_pgv = 0.8  # 0.2\n",
    "    C_pgd = 0.2  # 0.8\n",
    "    im = (C_pgv * df['numpgvrpr'] + C_pgd * df['numpgdrpr']).sum()/df['length'].sum()\n",
    "    SI_leak = 1-stats.lognorm(s=sigma, scale=math.exp(mu)).cdf(im)\n",
    "    \n",
    "    m = df_WDSlinks['linknwid'] == idx\n",
    "    df_WDSlinks.loc[m, ['SI_break_idv']] = SI_break\n",
    "    df_WDSlinks.loc[m, ['SI_leak__idv']] = SI_leak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Restoration and Functionality Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1) NetworkX Modeling of EPN and WDS networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform restoration and functionality analysis of infrastruture, first a directed graph model of the EPN and WDS, given by G_EPN and G_WDS, is developed. Subsequently, these two networkX graphs are combined to obtain an interdependednt graph of two networks given by G_EPN_WDS; this graph can be represented by an $n×n$ adjacency matrix $M(G)=[m_{ij}]$. [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### shp_to_network functions takes node and edge shapefiles of an infrastruture network\n",
    "### and constructs a networkX graph give by G\n",
    "\n",
    "def shp_to_network(df_network_nodes, df_network_links, sort = 'unsorted'):\n",
    "    \n",
    "    G=nx.DiGraph() #Empty graph\n",
    "\n",
    "    X = df_network_nodes['geometry'].apply(lambda p: p.x).head()\n",
    "    Y = df_network_nodes['geometry'].apply(lambda p: p.y).head()\n",
    "    ID = df_network_nodes['nodenwid']\n",
    "\n",
    "    pos = {}\n",
    "    X = df_network_nodes['geometry'].apply(lambda p: p.x)\n",
    "    Y = df_network_nodes['geometry'].apply(lambda p: p.y)\n",
    "    for i, val in enumerate(df_network_nodes[\"nodenwid\"]):\n",
    "        pos[val] = (X[i],Y[i])\n",
    "\n",
    "    edges = [(x,y) for x, y in zip(df_network_links[\"fromnode\"],df_network_links[\"tonode\"])]\n",
    "    edge = []\n",
    "\n",
    "    if sort == 'sorted':\n",
    "        for i, val in enumerate(df_network_links[\"linknwid\"]):\n",
    "            if df_network_links[\"direction\"][i] == 1:\n",
    "                edge.append((df_network_links[\"fromnode\"][i],df_network_links[\"tonode\"][i]))\n",
    "            else:\n",
    "                edge.append((df_network_links[\"tonode\"][i],df_network_links[\"fromnode\"][i]))\n",
    "    elif sort == 'unsorted':\n",
    "        for i, val in enumerate(df_network_links[\"linknwid\"]):\n",
    "            edge.append((df_network_links[\"fromnode\"][i],df_network_links[\"tonode\"][i]))        \n",
    "\n",
    "    G.add_nodes_from(pos.keys())\n",
    "    G.add_edges_from(edge)\n",
    "    for x, y, id in zip(X,Y,ID):\n",
    "        G.nodes[id]['pos'] = (x,y)\n",
    "\n",
    "    for ii,ID in enumerate(G.nodes()):\n",
    "        G.nodes[ID][\"classification\"] = df_network_nodes[\"utilfcltyc\"][ii]\n",
    "    \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain networkX representation of EPN and WDS networks\n",
    "G_EPN = shp_to_network(df_EPNnodes,df_EPNlinks)\n",
    "G_WDS = shp_to_network(df_WDSnodes,df_WDSlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add 100 to node and link IDs of EPN geodataframe to distinguish its node IDs from WDS\n",
    "df_EPNlinks['linknwid'] = df_EPNlinks['linknwid'] + 100\n",
    "df_EPNlinks['fromnode'] = df_EPNlinks['fromnode'] + 100\n",
    "df_EPNlinks['tonode'] = df_EPNlinks['tonode'] + 100\n",
    "df_EPNnodes['nodenwid'] = df_EPNnodes['nodenwid'] + 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2) Restoration Analysis of EPN Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restoration curves for electric substations presented in Hazus [5] are emplyed to perform retoration analysis of EPN substations. These functions are presented in Table 8-28 which provide  approximate discrete functions for the restoration of EPN components."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from pyincore import RestorationService\n",
    "from pyincore.analyses.epfrestoration import EpfRestoration"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epf_rest = EpfRestoration(client)\n",
    "# Load restoration mapping\n",
    "restorationsvc = RestorationService(client)\n",
    "mapping_set = MappingSet(restorationsvc.get_mapping(\"61f302e6e3a03e465500b3eb\"))  # new format of mapping\n",
    "epf_rest.set_input_dataset(\"epfs\", epn_dataset)\n",
    "epf_rest.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "epf_rest.set_input_dataset(\"damage\", substation_dmg_result)\n",
    "# result_name = fp + \"4_MMSA_epf_restoration_result\"\n",
    "epf_rest.set_parameter(\"result_name\", \"epf_restoration.csv\")\n",
    "epf_rest.set_parameter(\"restoration_key\", \"Restoration ID Code\")\n",
    "epf_rest.set_parameter(\"end_time\", 90.0)\n",
    "epf_rest.set_parameter(\"time_interval\", 1.0)\n",
    "# epf_rest.set_parameter(\"pf_interval\", 0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epf_rest.run_analysis()\n",
    "\n",
    "# Discretized Restoration functionality\n",
    "func_results = epf_rest.get_output_dataset(\"func_results\").get_dataframe_from_csv()\n",
    "func_results.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epf_time_results = epf_rest.get_output_dataset(\"time_results\").get_dataframe_from_csv()\n",
    "epf_time_results = epf_time_results.loc[(epf_time_results['time']==1)|(epf_time_results['time']==3)|(epf_time_results['time']==7)|(epf_time_results['time']==30)|(epf_time_results['time']==90)]\n",
    "epf_time_results.insert(2, 'PF_00', list(np.ones(len(epf_time_results)))) # PF_00, PF_0, PF_1, PF_2, PF_3  ---> DS_0, DS_1, DS_2, DS_3, DS_4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_EPN_node = pd.merge(df_EPNnodes[['nodenwid','utilfcltyc','guid']], df_substation_fail[['guid','DS_0', 'DS_1', 'DS_2', 'DS_3','DS_4', 'failure_probability']], on='guid', how='outer')\n",
    "epf_inventory_restoration_map = epf_rest.get_output_dataset(\"inventory_restoration_map\").get_dataframe_from_csv()\n",
    "EPPL_restoration_id = list(epf_inventory_restoration_map.loc[epf_inventory_restoration_map['guid']==df_EPN_node.loc[df_EPN_node['utilfcltyc']=='EPPL'].guid.tolist()[0]]['restoration_id'])[0]\n",
    "ESS_restoration_id = list(set(epf_inventory_restoration_map.restoration_id.unique())-set([EPPL_restoration_id]))[0]\n",
    "df_EPN_node_EPPL = df_EPN_node.loc[df_EPN_node['utilfcltyc']=='EPPL']\n",
    "df_EPN_node_ESS = df_EPN_node.loc[df_EPN_node['utilfcltyc']!='EPPL']\n",
    "epf_time_results_EPPL = epf_time_results.loc[epf_time_results['restoration_id']==EPPL_restoration_id][['PF_00', 'PF_0', 'PF_1', 'PF_2', 'PF_3']]\n",
    "EPPL_func_df = pd.DataFrame(np.dot(df_EPN_node_EPPL[['DS_0', 'DS_1', 'DS_2', 'DS_3', 'DS_4']], np.array(epf_time_results_EPPL).T), columns=['functionality1', 'functionality3', 'functionality7', 'functionality30', 'functionality90'])\n",
    "EPPL_func_df.insert(0, 'guid', list(df_EPN_node_EPPL.guid))\n",
    "epf_time_results_ESS = epf_time_results.loc[epf_time_results['restoration_id']==ESS_restoration_id][['PF_00', 'PF_0', 'PF_1', 'PF_2', 'PF_3']]\n",
    "ESS_func_df = pd.DataFrame(np.dot(df_EPN_node_ESS[['DS_0', 'DS_1', 'DS_2', 'DS_3', 'DS_4']], np.array(epf_time_results_ESS).T), columns=['functionality1', 'functionality3', 'functionality7', 'functionality30', 'functionality90'])\n",
    "ESS_func_df.insert(0, 'guid', list(df_EPN_node_ESS.guid))\n",
    "epf_function_df = pd.concat([ESS_func_df, EPPL_func_df], ignore_index=True)\n",
    "df_EPN_node = pd.merge(df_EPN_node, epf_function_df, on=\"guid\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_EPN_node.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4-3) Restoration Analysis of WDS Components"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Restoration curves for WDS presented in Hazus are emplyed to perform retoration analysis of WDS nodes. These functions are presented in Table 8-2, which provide  approximate discrete functions for the restoration of Potable Water System Components."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_WDS_node = pd.merge(df_WDSnodes[['nodenwid','utilfcltyc','guid']], df_waterfacility_dmg[['guid', 'DS_0', 'DS_1', 'DS_2', 'DS_3','DS_4']], on='guid', how='outer')\n",
    "\n",
    "### Discretized Restoration Functions for 'PSTAS': Above Ground Steel Tank\n",
    "PSTASrest_dict = {1:  [100,  30,  20,  13,  10],\n",
    "                  3:  [100, 100,  49,  15,  11],\n",
    "                  7:  [100, 100,  93,  16,  12],\n",
    "                 30:  [100, 100, 100,  23,  15],\n",
    "                 90:  [100, 100, 100,  40,  30]}\n",
    "\n",
    "### Discretized Restoration Functions for 'PPPL': Large Pump Plant\n",
    "PPPLrest_dict = {1:  [100,  65,  22,  10,   3],\n",
    "                 3:  [100, 100,  50,  15,   4],\n",
    "                 7:  [100, 100, 100,  25,   6],\n",
    "                30:  [100, 100, 100,  95,  40],\n",
    "                90:  [100, 100, 100, 100, 100]}\n",
    "\n",
    "for key in [1, 3, 7, 30, 90]:\n",
    "    def f(row):\n",
    "        if row['utilfcltyc'] == 'PSTAS':\n",
    "            C0, C1, C2, C3, C4 = PSTASrest_dict[key][0],PSTASrest_dict[key][1],PSTASrest_dict[key][2],PSTASrest_dict[key][3],PSTASrest_dict[key][4]\n",
    "            func = (C0*row['DS_0'] + C1*row['DS_1'] + C2*row['DS_2'] + C3*row['DS_3'] + C4*row['DS_4'])/100\n",
    "            return func\n",
    "        elif row['utilfcltyc'] == 'PPPL':\n",
    "            C0, C1, C2, C3, C4 = PPPLrest_dict[key][0],PPPLrest_dict[key][1],PPPLrest_dict[key][2],PPPLrest_dict[key][3],PPPLrest_dict[key][4]\n",
    "            func = (C0*row['DS_0'] + C1*row['DS_1'] + C2*row['DS_2'] + C3*row['DS_3'] + C4*row['DS_4'])/100\n",
    "            return func\n",
    "    df_WDS_node['functionality'+str(key)] = df_WDS_node.apply(f, axis=1)\n",
    "df_WDS_node.head()    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyincore.analyses.waterfacilityrestoration import WaterFacilityRestoration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wf_rest = WaterFacilityRestoration(client)\n",
    "# Load restoration mapping\n",
    "mapping_set = MappingSet(restorationsvc.get_mapping(\"61f075ee903e515036cee0a5\"))  # new format of mapping\n",
    "wf_rest.load_remote_input_dataset(\"water_facilities\", \"5a284f2ac7d30d13bc081e52\")  # water facility\n",
    "wf_rest.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "wf_rest.set_input_dataset(\"damage\", waterfacility_dmg_result)\n",
    "wf_rest.set_parameter(\"result_name\", \"wf_restoration\")\n",
    "wf_rest.set_parameter(\"restoration_key\", \"Restoration ID Code\")\n",
    "wf_rest.set_parameter(\"end_time\", 90.0)\n",
    "wf_rest.set_parameter(\"time_interval\", 1.0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wf_rest.run_analysis()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wf_time_results = wf_rest.get_output_dataset(\"time_results\").get_dataframe_from_csv()\n",
    "wf_time_results = wf_time_results.loc[(wf_time_results['time']==1)|(wf_time_results['time']==3)|(wf_time_results['time']==7)|(wf_time_results['time']==30)|(wf_time_results['time']==90)]\n",
    "wf_time_results.insert(2, 'PF_00', list(np.ones(len(wf_time_results))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_WDS_node = pd.merge(df_WDSnodes[['nodenwid','utilfcltyc','guid']], df_waterfacility_dmg[['guid', 'DS_0', 'DS_1', 'DS_2', 'DS_3','DS_4']], on='guid', how='outer')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wf_inventory_restoration_map = wf_rest.get_output_dataset(\"inventory_restoration_map\").get_dataframe_from_csv()\n",
    "PPPL_restoration_id = list(wf_inventory_restoration_map.loc[wf_inventory_restoration_map['guid']==df_WDS_node.loc[df_WDS_node['utilfcltyc']=='PPPL'].guid.tolist()[0]]['restoration_id'])[0]\n",
    "PSTAS_restoration_id = list(wf_inventory_restoration_map.loc[wf_inventory_restoration_map['guid']==df_WDS_node.loc[df_WDS_node['utilfcltyc']=='PSTAS'].guid.tolist()[0]]['restoration_id'])[0]\n",
    "df_WDS_node_PPPL = df_WDS_node.loc[df_WDS_node['utilfcltyc']=='PPPL']\n",
    "df_WDS_node_PSTAS = df_WDS_node.loc[df_WDS_node['utilfcltyc']=='PSTAS']\n",
    "\n",
    "wf_time_results_PPPL = wf_time_results.loc[wf_time_results['restoration_id']==PPPL_restoration_id][['PF_00', 'PF_0', 'PF_1', 'PF_2', 'PF_3']]\n",
    "PPPL_func_df = pd.DataFrame(np.dot(df_WDS_node_PPPL[['DS_0', 'DS_1', 'DS_2', 'DS_3', 'DS_4']], np.array(wf_time_results_PPPL).T), columns=['functionality1', 'functionality3', 'functionality7', 'functionality30', 'functionality90'])\n",
    "PPPL_func_df.insert(0, 'guid', list(df_WDS_node_PPPL.guid))\n",
    "\n",
    "wf_time_results_PSTAS = wf_time_results.loc[wf_time_results['restoration_id']==PSTAS_restoration_id][['PF_00', 'PF_0', 'PF_1', 'PF_2', 'PF_3']]\n",
    "PSTAS_func_df = pd.DataFrame(np.dot(df_WDS_node_PSTAS[['DS_0', 'DS_1', 'DS_2', 'DS_3', 'DS_4']], np.array(wf_time_results_PSTAS).T), columns=['functionality1', 'functionality3', 'functionality7', 'functionality30', 'functionality90'])\n",
    "PSTAS_func_df.insert(0, 'guid', list(df_WDS_node_PSTAS.guid))\n",
    "\n",
    "wf_function_df = pd.concat([PSTAS_func_df, PPPL_func_df], ignore_index=True)\n",
    "df_WDS_node = pd.merge(df_WDS_node, wf_function_df, on=\"guid\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_WDS_node.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4-4) NetworkX modeling of interdependent EPN and WDS networks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4-4-A) The EPN and WDS pecentage functionality dataframes are combined in this section to obtain an interdependent networkX model of EPN and WDS."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_functionality_nodes = df_EPN_node.append(df_WDS_node, ignore_index=True)\n",
    "df_network_nodes = df_EPNnodes.append(df_WDSnodes, ignore_index=True)\n",
    "df_network_links = df_EPNlinks.append(df_WDSlinks, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Obtain an interdepentend graph model of networks\n",
    "G_WDS_EPN_Full = shp_to_network(df_network_nodes,df_network_links)\n",
    "G_EPN_WDS_Full = shp_to_network(df_network_nodes,df_network_links)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4-4-B) Assign SI values obtained from pipeline repair analysis as weight to networkX model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nodes in df_WDS_node['nodenwid']:\n",
    "    if len(list(G_WDS_EPN_Full.predecessors(nodes))) > 0:\n",
    "        for node in list(G_WDS_EPN_Full.predecessors(nodes)):\n",
    "            weight = df_WDSlinks[df_WDSlinks['fromnode']==node].SI_break_idv.values[0]\n",
    "            G_WDS_EPN_Full[node][nodes]['weight'] = weight\n",
    "            G_EPN_WDS_Full[node][nodes]['weight'] = weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4-4-C) Define the dependency between EPN and WDS with additional edges within the networkX model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "origin_EPN = [113,116,125,127,128,132,135,137,139]\n",
    "destination_WDS  = [4  ,5  ,12 ,2  ,3  ,6  ,8  ,9  ,11]\n",
    "for i, j in zip(origin_EPN, destination_WDS):\n",
    "    G_WDS_EPN_Full.add_edge(i,j)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "origin_WDS = [ 21,  25,  23 ,  29,  30,  35,  39, 42]\n",
    "destination_EPN  = [101, 102, 103 , 104, 105, 106, 107, 108]\n",
    "for i, j in zip(origin_WDS,destination_EPN):\n",
    "    G_EPN_WDS_Full.add_edge(i,j)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5) Implemention of an Input-Output model to estimate the functionality of EPN nodes by accounting for dependency between EPN and WDS networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EPN_Bldg_Depend_MC = df_EPN_Bldg_Depend.copy()\n",
    "\n",
    "columns = ['guid', 'sguid', 'nodenwid', 'func_cascading1','func_cascading3', 'func_cascading7', 'func_cascading30','func_cascading90']\n",
    "df_EPN_Bldg_functionality =  df_EPN_Bldg_Depend[['guid', 'sguid', 'nodenwid', 'func_cascading1',\n",
    "       'func_cascading3', 'func_cascading7', 'func_cascading30',\n",
    "       'func_cascading90']].copy()\n",
    "for col in columns[3:]:\n",
    "    df_EPN_Bldg_functionality[col].values[:] = 0\n",
    "\n",
    "no_samples = 100\n",
    "for MC_sample in range(0,no_samples):\n",
    "    for sguid in df_EPN_Bldg_Depend_MC.sguid.unique():\n",
    "        ss_row  = df_ss_functionality.loc[df_ss_functionality['sguid'] == sguid]\n",
    "        bldg_rows = df_EPN_Bldg_Depend_MC.loc[df_EPN_Bldg_Depend_MC['sguid'] == sguid]\n",
    "        for idx in [1, 3, 7, 30, 90]:\n",
    "            day = str(idx)\n",
    "            percent_functional = ss_row['func_cascading'+day].values[0]\n",
    "            percent_functional = percent_functional if percent_functional<1 else 1\n",
    "            nums = np.random.choice([1, 0], size=len(bldg_rows), p=[percent_functional, 1-percent_functional])\n",
    "            df_EPN_Bldg_Depend_MC.loc[df_EPN_Bldg_Depend_MC['sguid'] == sguid, 'func_cascading'+day] = nums\n",
    "    for idx in [1, 3, 7, 30, 90]:\n",
    "        day = str(idx)\n",
    "        df_EPN_Bldg_functionality['func_cascading'+day] = df_EPN_Bldg_functionality['func_cascading'+day] + (1/no_samples)*df_EPN_Bldg_Depend_MC['func_cascading'+day] * df_EPN_Bldg_Depend_MC['bldg_functionality']\n",
    "    \n",
    "df_EPN_Bldg_functionality\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge EPN functionality estimates with EPN shapefile for plotting\n",
    "df_EPN_Bldg_functionality_gdf = bldg_gdf.merge(df_EPN_Bldg_functionality, on = 'guid', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-6) Results obtain for functionality of buildings by accounting for the interdependency between buildings and EPN network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example results obtain for functionality of buildings 1 day, 7 days and 30 days after an event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Day After Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_gdf_map(df_EPN_Bldg_functionality_gdf, 'func_cascading1', basemap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 Days After Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_gdf_map(df_EPN_Bldg_functionality_gdf, 'func_cascading7', basemap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30 Days After Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_gdf_map(df_EPN_Bldg_functionality_gdf, 'func_cascading30', basemap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of buildings having functionality estimates greater or equal 0.5 days after event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EPN_Bldg_functionality[columns[3:]][df_EPN_Bldg_functionality[columns[3:]] >= 0.5].count()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d112d8375bf01ae4f4972c8b46ca9ea510a0ab21a221189d40a5cb1c5f1a75f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}